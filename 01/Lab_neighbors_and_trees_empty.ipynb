{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1600348232748,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "lXyR3B1-cst0",
    "outputId": "11ecde63-acd0-40e3-9ec5-d1687691f077"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McTkC40Tcst3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lopz8Y9Ccst6"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8pLlMwKcst8"
   },
   "source": [
    "## Degree in Data Science and Engineering, group 96\n",
    "## Machine Learning 2\n",
    "### Fall 2023\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "# Lab 1. Neighbors & Trees\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "**Emilio Parrado Hern√°ndez**\n",
    "\n",
    "Dept. of Signal Processing and Communications\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "data.target[[10, 50, 85]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZAyM0Hscst-"
   },
   "source": [
    "# A real world problem: Breast Cancer\n",
    "\n",
    "## UCI repository of ML datasets\n",
    "[Breast Cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) is one of the classic benchmark problems in the [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php). In general, the usual way to compare the performances of different machine learning algorithms consists in to evaluate their performances in solving bechmark problems. There is another approximation, the field of *Statistical Learning Theory* (SLT), that tries to analize these performances through the use of **bounds** on the generalization capabilities of these algorithms. Being pragmatical, although SLT seems a quite more robust approach, real experience teaches that the estimation of performances based on *benchmarks* predict more accurately what practitioners experience when they put this algorithms to work with real data.\n",
    "\n",
    "The UCI repository is a key reference for the design and development of general purpose machine learning algorithms, as it enables to esily gather intuitions about the performance of such algorithms in different situations.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQyS7-Kzcst-"
   },
   "source": [
    "# 1. Loading data\n",
    " \n",
    "1. Read data (from file, database)\n",
    "2. Separate observations from *targets*\n",
    "3. Divide data into two sets: **training** and **test**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A6S90xBcst_"
   },
   "source": [
    "## 1.1. Read data\n",
    "\n",
    "This dataset can be directly loaded from sklearn. First read the description of the task and features here https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 787,
     "status": "ok",
     "timestamp": 1600348253538,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "aIgJdjQGcst_",
    "outputId": "adcac967-04de-4807-acfb-1fd5f7654549"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "# read data from sklearn and create a pandas dataframe\n",
    "data_dic = load_breast_cancer()\n",
    "data = data_dic['data'] # the observations\n",
    "print(\"The data set is formed by {0:d} observations in {1:d} dimensions\".format(data.shape[0], data.shape[1]))\n",
    "targets = data_dic['target'] # the targets\n",
    "columns = [cc for cc in data_dic['feature_names']] +['target']\n",
    "data_df = pd.DataFrame(np.vstack((data.T, targets)).T, columns=columns) \n",
    "data_df.head() # print 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1600348303060,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "oEpqPlEocsuC",
    "outputId": "f3820d29-84a4-470a-c4d8-8c5c281354c2"
   },
   "outputs": [],
   "source": [
    "data_df.describe() # print simple statistics of all the rows in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUJKv1BXcsuE"
   },
   "source": [
    "## 1.2. Separate observations from targets\n",
    "\n",
    "In this case this is already done by sklearn. The method [`load_breast_cancer`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) returns a python dictionary where key `data` contains the observations and key `target` their corresponding targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqUM2rdPcsuH"
   },
   "source": [
    "## 1.3. Divide data into training and test sets\n",
    "\n",
    "The key indicator of the good performance of a machine learning model is its **generalization capability**. This means that the model outputs correct inferences about data not used during the training phase. A common way of addressing this point is to split the data into two disjoint partitions:\n",
    "- the **training set**, observations used by the **training algorithm** to optimize the model (remember, fix values to the free parameters of the model)\n",
    "\n",
    "- the **test set** is a separate set that is processed with an  **already trained model**. We use the test set to assess the **generalization capabilities** of the model. A model presents a good generalization when its performance in the test set is similar to its performance in the training set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecsViB5IcsuH"
   },
   "source": [
    "Anyway, don't forget that:\n",
    "- **the test set is just another set**. It means that when we eventually put the model **in production** we still need to monitor its performance with the different test sets that we will be getting (every day, hour, week, etc)\n",
    "- Sometimes we try to refine a model  **using the performance in the test sets**. This too usual practice introduces biases in the estimation of the performances of the model as we are  **feeding back information from the test**. Somehow this means that the test is taking part in the training, so the test set can't be considered 100% independent from the training process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnQTLwhFcsuI"
   },
   "source": [
    "In some datasets the training/test split is already defined. In cases where the data comes in a single set (like this Breat Cancer problem) the data scientist needs to propose a division. It is common to leave a larger proportion of data for training purposes than for test. Usual sizes for these sets are\n",
    "- 50% training, 50% test\n",
    "- 70% training, 30% test\n",
    "- 80% training, 20% test\n",
    "\n",
    "The trade-off you have to take into account is the following:\n",
    "- A larger training data means you optimizer will have more information to find suitable values for the parameters. You need a significantly larger number of data compared to the number of free parameters of the model in order to experience a robust learning.\n",
    "- A larger test set means that your estimation of the generalization capabilities of the model with new, unseen data will be more reliable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCnsTFXEgn4e"
   },
   "source": [
    "Sklearn has a built-in method to carry out the splitting of a data set in training and test sets: [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Explore it to split your data in a 70% for training, 30% for testing partition. Fix the value of parameter `random_state` to ease reproducibility (I usually use 42)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDHXxMfIcsuJ"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X1qK-dPcsuK"
   },
   "source": [
    "# 2. First models\n",
    "\n",
    "The first step is to start exploring solutions by learning simple models. We use the 3 families that were reviewed in the lecture: kNN, Decision Trees and Random Forest.\n",
    "\n",
    "Before starting with the learnings with must decide a metric to evaluate the performance of the algorithms. We will use the **classifiction accuracy**  (discussed also in the lecture) as it is the default score in sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5f6yNC6jrG5"
   },
   "source": [
    "## 2.1 First model with $k$NN\n",
    "\n",
    "Explore different combinations of the **number of neighbors** $k$ **with and without weighting** the vote of each neighbor. \n",
    "- Use plots of accuracy versus $k$ to present the results\n",
    "- Decide the configuration of the best model and print the accuracy achieved by this best model in the training and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hm1JjHVccsus"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#range for the number of neighbors to be explored\n",
    "v_nn = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100,200,350]\n",
    "\n",
    "# store the accuracy predicting the training set with uniform weighting of votes\n",
    "acc_entr = np.empty(len(v_nn))\n",
    "\n",
    "# store the accuracy predicting the test set with uniform weighting of votes\n",
    "acc_test = np.empty(len(v_nn))\n",
    "\n",
    "# store the accuracy predicting the training set with votes weighted by inverse distance\n",
    "acc_entr_w = np.empty(len(v_nn))\n",
    "\n",
    "# store the accuracy predicting the test set with votes weighted by inverse distance\n",
    "acc_test_w = np.empty(len(v_nn))\n",
    "\n",
    "\n",
    "#main loop\n",
    "for inn, n_neighbors in enumerate(v_nn):\n",
    "    #instantiate model with uniform voting\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    #train model\n",
    "    knn.fit(x_train, y_train)\n",
    "    #compute scores\n",
    "    acc_entr[inn] = knn.score(x_train, y_train)\n",
    "    acc_test[inn] = knn.score(x_test, y_test)\n",
    "    #instantiate model with weightd voting\n",
    "    knn_w = KNeighborsClassifier(n_neighbors=n_neighbors, weights='distance')\n",
    "    #train model\n",
    "    knn_w.fit(x_train, y_train)\n",
    "    #compute scores\n",
    "    acc_entr_w[inn] = knn_w.score(x_train, y_train)\n",
    "    acc_test_w[inn] = knn_w.score(x_test, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xnb5BbYzRhx2"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# CODE FOR PLOTTING #\n",
    "#####################\n",
    "plt.figure()\n",
    "plt.plot(v_nn, acc_entr, label='Acc. train')\n",
    "plt.plot(v_nn, acc_test, label='Acc test')\n",
    "plt.plot(v_nn, acc_entr_w, label='Acc. train, weighted')\n",
    "plt.plot(v_nn, acc_test_w, label='Acc. test, weighted')\n",
    "_ = plt.xlabel('k')\n",
    "_ = plt.ylabel('Acc.')\n",
    "_ = plt.legend()\n",
    "\n",
    "plt.grid()\n",
    "best_k = v_nn[np.argmax(acc_test)]\n",
    "plt.show()\n",
    "print(\"Best k, unit vote for the test set is {0:d}, Acc of  {1:.2f}\".format(best_k, np.max(acc_test)))\n",
    "best_k_w = v_nn[np.argmax(acc_test_w)]\n",
    "plt.show()\n",
    "print(\"Best k, weighted vote for the test set is {0:d}, Acc of  {1:.2f}\".format(best_k_w, np.max(acc_test_w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PtBDWO5csvL"
   },
   "source": [
    "## 2.2 First model with Decision Trees\n",
    "\n",
    "Explore different combinations of the **maximum number of leaf nodes** $k$ \n",
    "- Use plots of accuracy versus the maximum number of leaves to present the results\n",
    "- Decide the configuration of the best model and print the accuracy achieved by this best model in the training and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhV-rOx5csvR"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZxkYsmjcsvS"
   },
   "source": [
    "## 2.3. First model with Random Forests\n",
    "\n",
    "Explore different combinations of the **number of trees in the forest** and of the **maximum number of leaves in each tree**. \n",
    "- For the maximum number of leaves, use just 3 values: \n",
    "  - 3\n",
    "  - the number you selected as better choice for the decision tree\n",
    "  - A reasonable number between 3 and the second choice (look at the accuracy vs number of leaves plot to get insights for this choice).\n",
    "\n",
    "- Use plots of accuracy versus the number of trees to present the results\n",
    "- Hint: explore the number of trees using logarithmic jumps between 1 and 1000\n",
    "- Decide the configuration of the best model and print the accuracyachieved by this best model in the training and in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMpIxsZMcsvf"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc-c2ywtvnYA"
   },
   "source": [
    "# 3. Simple transformations of features\n",
    "\n",
    "Normalization of the features is one of the most used pre-processing techniques in machine learning. If we consider each feature a **random variable**, the normalization transforms it into a random variable with **zero mean** and **unit variance**.\n",
    "$$\n",
    "x_i \\longrightarrow \\frac{x_i - \\mathbb E\\{x\\}}{\\mbox{std dev}\\{x\\}}\n",
    "$$\n",
    "There is a sklearn module that implements normalization for us: [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "\n",
    "`StandardScaler` basic methods `fit` and `transform` will do the work for us:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCxNIf7LO1pF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() #instantiate\n",
    "scaler.fit(x_train)  # fit with training data\n",
    "x_train_s = scaler.transform(x_train)\n",
    "x_test_s = scaler.transform(x_test)\n",
    "print(\"Training set\")\n",
    "print(\"Means before -> Means after\")\n",
    "for ii in range(x_train.shape[1]):\n",
    "  print(\"{0:.3f}  ->  {1:.3f}\".format(x_train[:,ii].mean(0),x_train_s[:,ii].mean(0)))\n",
    "print(\"\")\n",
    "print(\"Std dev before -> Std dev after\")\n",
    "for ii in range(x_train.shape[1]):\n",
    "  print(\"{0:.3f}  ->  {1:.3f}\".format(x_train[:,ii].std(0),x_train_s[:,ii].std(0)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Test set\")\n",
    "print(\"Means before -> Means after\")\n",
    "for ii in range(x_test.shape[1]):\n",
    "  print(\"{0:.3f}  ->  {1:.3f}\".format(x_test[:,ii].mean(0),x_test_s[:,ii].mean(0)))\n",
    "print(\"\")\n",
    "print(\"Std dev before -> Std dev after\")\n",
    "for ii in range(x_train.shape[1]):\n",
    "  print(\"{0:.3f}  ->  {1:.3f}\".format(x_test[:,ii].std(0),x_test_s[:,ii].std(0)))\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wqVj2R1-0rS"
   },
   "source": [
    "# REMEMBER TO FIT YOUR SCALER ONLY WITH TRAINING DATA\n",
    "# DO NOT USE TEST DATA TO FIT THE SCALER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K0hfjGM-_Z8"
   },
   "source": [
    "## 3.1 $k$NN with normalized data\n",
    "Repeat the study in section 2.1 using the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2H10gGpM8IGM"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iuucz8x3_mEp"
   },
   "source": [
    "## 3.2 Decision Trees with normalized data\n",
    "Repeat the study in section 2.2 using the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhEwBGv3_eO4"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3ze-A7m__Jt"
   },
   "source": [
    "## 3.3 Random Forests with normalized data\n",
    "Repeat the study in section 2.3 using the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAVxn220_zSf"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J7P3Y9wAtmv"
   },
   "source": [
    "# 4. Pipelines\n",
    "\n",
    "Scikit learn provides with a easy and clean way to automatize the scaling before the use of a machine learning method, the [**pipelines**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Read the documentation about the use of pipelines and understand the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iGBszYH_9WY"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# definition of the pipeline with the list of methods to be connected\n",
    "# each member of the list is a tuple: ('name of the stage', StageConstructorMethod(arg1, arg2, ...))\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('kNN', KNeighborsClassifier(n_neighbors=2, weights='distance'))])\n",
    "\"\"\"\n",
    "Fitting the pipeline performs a sequential invocation of the fit methods of \n",
    "all the connected stages. The output of the previous stage serves as input for\n",
    "the next one.\n",
    "\"\"\"\n",
    "pipe.fit(x_train, y_train)\n",
    "\n",
    "# evaluation of the scaler + regressor\n",
    "train_risk = pipe.score(x_train, y_train)\n",
    "test_risk = pipe.score(x_test, y_test)\n",
    "print(\"Acc in the training set after scaling: {0:.2f}\".format(train_risk))\n",
    "print(\"Acc in the test set after scaling: {0:.2f}\".format(test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Lnajd5JQvI"
   },
   "source": [
    "# 5. MinMax Scaler\n",
    "Another alternative to the normalization is to scale each feature so that its range of values lays between 0 and 1. Scikit Learn module [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) does this job for us.\n",
    "\n",
    "Repeat sections 3.1, 3.2 and 3.3 but using a MinMax Scaler instead of a StandardScaler and connecting scaler and regressor with a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWyv6BaIJAr_"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKBJRqBkMFH3"
   },
   "source": [
    "# 6. Wrapping up\n",
    "Discussion, general results\n",
    "- What was the best model to solve the Breast Cancer problem?\n",
    "- How significant are the differences in performance?\n",
    "- Which is the impact of scaling the features in the three methods?\n",
    "- Is there any significant difference in performance in the two scalers?\n",
    "\n",
    "About $kNN$\n",
    "- Discuss the impact of scaling.\n",
    "- How is the behavior of $k$NN as $k$ increases?\n",
    "\n",
    "About Decision Trees\n",
    "- Impact of scaling.\n",
    "- Grow and draw a tree with just 4 or 5 leaf nodes and discuss if the features used for these first splits make sense\n",
    "\n",
    "About Random Forest\n",
    "- Impact of scaling the features\n",
    "- Discuss how varies the performance with the number of leaf nodes per tree and the size of the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZvD1uTkK813"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "DgU2fdCqcsuh",
    "8d2odd9rcsui",
    "g0Ea-0-Ncsur",
    "aKNj2quWcsu2",
    "TG6Hczj5csu2",
    "hkaoPI7Ncsu2",
    "xnQzHuwfcsvL",
    "tkhy0RCbcsvS",
    "HjTktM4tcsvT",
    "6QBX3xt8csvT"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
