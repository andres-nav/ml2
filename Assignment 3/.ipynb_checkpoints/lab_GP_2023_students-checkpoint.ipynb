{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1604482757442,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "lXyR3B1-cst0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1604482757927,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "McTkC40Tcst3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1473,
     "status": "ok",
     "timestamp": 1604482757927,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "lopz8Y9Ccst6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8pLlMwKcst8"
   },
   "source": [
    "## Degree in Data Science and Engineering, group 96\n",
    "## Machine Learning 2\n",
    "### Fall 2023\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "# Lab 4. Gaussian Processes\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "**Emilio Parrado Hern√°ndez**\n",
    "\n",
    "Dept. of Signal Processing and Communications\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqqigYrw4WXW"
   },
   "source": [
    "# Diabetes dataset\n",
    "\n",
    "[Diabetes](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset) is another classic benchmark for regression. Each observation corresponds to a diabetes patient represented by 10 variables and the corresponding target is a score that measures  the disease progression one year after baseline.\n",
    "\n",
    "The variables that form each observation are:\n",
    "- age in years\n",
    "\n",
    "- sex\n",
    "\n",
    "- bmi body mass index\n",
    "\n",
    "- bp average blood pressure\n",
    "\n",
    "- six measures taken from the blood of the patient:\n",
    "  - s1 tc, T-Cells (a type of white blood cells)\n",
    "\n",
    "  - s2 ldl, low-density lipoproteins\n",
    "\n",
    "  - s3 hdl, high-density lipoproteins\n",
    "\n",
    "  - s4 tch, thyroid stimulating hormone\n",
    "\n",
    "  - s5 ltg, lamotrigine\n",
    "\n",
    "  - s6 glu, blood sugar level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2256,
     "status": "ok",
     "timestamp": 1604482758714,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "aIgJdjQGcst_",
    "outputId": "beaf2ca4-ad23-424a-f1f6-8f42ab8fd18c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names are\n",
      "Column 0: AGE\n",
      "Column 1: SEX\n",
      "Column 2: BMI\n",
      "Column 3: BP\n",
      "Column 4: TC\n",
      "Column 5: LDL\n",
      "Column 6: HDL\n",
      "Column 7: TCH\n",
      "Column 8: LTG\n",
      "Column 9: GLU\n",
      "\n",
      "Loaded 442 observations with 10 columns\n",
      "Loaded 442 targets\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('diabetes.csv', header=0)\n",
    "data.columns = ['AGE', 'SEX', 'BMI', 'BP','TC','LDL','HDL','TCH','LTG','GLU','Y']\n",
    "feature_names = data.columns # list with feature names\n",
    "print(\"Feature names are\")\n",
    "for ii,fn  in enumerate(feature_names[:-1]):\n",
    "    print(\"Column {0:d}: {1}\".format(ii,fn))\n",
    "X = data.values[:,:-1]\n",
    "Y = data['Y'].values\n",
    "print(\"\")\n",
    "print(\"Loaded {0:d} observations with {1:d} columns\".format(X.shape[0], X.shape[1]))\n",
    "print(\"Loaded {0:d} targets\".format(len(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI</th>\n",
       "      <th>BP</th>\n",
       "      <th>TC</th>\n",
       "      <th>LDL</th>\n",
       "      <th>HDL</th>\n",
       "      <th>TCH</th>\n",
       "      <th>LTG</th>\n",
       "      <th>GLU</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "      <td>442.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48.518100</td>\n",
       "      <td>1.468326</td>\n",
       "      <td>26.375792</td>\n",
       "      <td>94.647014</td>\n",
       "      <td>189.140271</td>\n",
       "      <td>115.439140</td>\n",
       "      <td>49.788462</td>\n",
       "      <td>4.070249</td>\n",
       "      <td>4.642700</td>\n",
       "      <td>91.260181</td>\n",
       "      <td>152.133484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.109028</td>\n",
       "      <td>0.499561</td>\n",
       "      <td>4.418122</td>\n",
       "      <td>13.831283</td>\n",
       "      <td>34.608052</td>\n",
       "      <td>30.413081</td>\n",
       "      <td>12.934202</td>\n",
       "      <td>1.290450</td>\n",
       "      <td>0.521877</td>\n",
       "      <td>11.496335</td>\n",
       "      <td>77.093005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.258100</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>164.250000</td>\n",
       "      <td>96.050000</td>\n",
       "      <td>40.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.276700</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.620050</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>140.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29.275000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>209.750000</td>\n",
       "      <td>134.500000</td>\n",
       "      <td>57.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.997200</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>211.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>42.200000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>242.400000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>9.090000</td>\n",
       "      <td>6.107000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>346.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              AGE         SEX         BMI          BP          TC         LDL  \\\n",
       "count  442.000000  442.000000  442.000000  442.000000  442.000000  442.000000   \n",
       "mean    48.518100    1.468326   26.375792   94.647014  189.140271  115.439140   \n",
       "std     13.109028    0.499561    4.418122   13.831283   34.608052   30.413081   \n",
       "min     19.000000    1.000000   18.000000   62.000000   97.000000   41.600000   \n",
       "25%     38.250000    1.000000   23.200000   84.000000  164.250000   96.050000   \n",
       "50%     50.000000    1.000000   25.700000   93.000000  186.000000  113.000000   \n",
       "75%     59.000000    2.000000   29.275000  105.000000  209.750000  134.500000   \n",
       "max     79.000000    2.000000   42.200000  133.000000  301.000000  242.400000   \n",
       "\n",
       "              HDL         TCH         LTG         GLU           Y  \n",
       "count  442.000000  442.000000  442.000000  442.000000  442.000000  \n",
       "mean    49.788462    4.070249    4.642700   91.260181  152.133484  \n",
       "std     12.934202    1.290450    0.521877   11.496335   77.093005  \n",
       "min     22.000000    2.000000    3.258100   58.000000   25.000000  \n",
       "25%     40.250000    3.000000    4.276700   83.250000   87.000000  \n",
       "50%     48.000000    4.000000    4.620050   91.000000  140.500000  \n",
       "75%     57.750000    5.000000    4.997200   98.000000  211.500000  \n",
       "max     99.000000    9.090000    6.107000  124.000000  346.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and test set\n",
    "\n",
    "Divide the data set into a training set with $3/4$ of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([166, 189, 173, 220, 206,  97,  60,  61, 242, 121, 128, 104, 265,\n",
       "       132, 283, 174, 129, 257, 137,  63,  93, 232, 208, 261, 179, 258,\n",
       "       262,  51, 237,  71, 139, 268,  69, 317, 249, 154, 192, 116,  81,\n",
       "       122, 259, 191, 292,  55, 107, 210,  91, 253,  85, 252,  59,  78,\n",
       "       200,  78, 245, 175,  42, 127,  53,  94, 104, 199, 265, 281, 248,\n",
       "       257, 215, 303, 170,  59, 277, 209, 138, 198, 124,  96, 288, 225,\n",
       "       265, 101,  55, 198,  51, 252,  64, 220, 131, 212, 142, 103, 155,\n",
       "       121,  86, 111,  65, 131,  51, 128, 141,  48, 109, 178,  88,  84,\n",
       "       216, 150,  60,  96, 190,  74, 279, 182, 160, 245, 276, 174, 180,\n",
       "       150, 196, 138,  97, 246, 321, 308, 109,  69, 182, 258, 161, 178,\n",
       "       214,  45, 150, 160,  55, 197, 185, 268, 310, 123,  68,  72, 185,\n",
       "       144, 147, 168, 178, 246, 151, 127,  83, 332, 152, 109,  90,  66,\n",
       "       214,  85, 129,  89, 259, 229, 200,  77,  54,  31, 109, 206, 144,\n",
       "       118,  83, 242, 259,  72, 163, 181, 141,  71, 137, 195, 179, 102,\n",
       "       131,  47, 235,  77, 198,  93, 162, 225, 275, 183, 306,  81,  55,\n",
       "       146, 196, 230, 310,  40, 135, 346,  43, 128,  77, 235,  49,  74,\n",
       "        92,  84, 263, 144, 142, 341, 115, 158, 273,  85,  88, 220,  39,\n",
       "        80, 172, 217, 336,  52, 272, 115, 110, 131,  71, 275, 118,  25,\n",
       "       100, 281, 221, 248, 200, 132,  91,  67, 202,  73,  85, 275, 243,\n",
       "        66, 293, 236, 243,  87,  39, 217,  92, 296, 292, 142,  50,  53,\n",
       "       104,  75, 120, 142, 143,  99,  65, 116, 233, 164,  95,  59, 139,\n",
       "       145, 177, 185,  97,  42, 201, 241,  70,  78,  49, 103,  44, 111,\n",
       "       191,  47, 182,  58, 155, 151,  79, 104, 143, 152, 170,  75, 200,\n",
       "       124,  91,  49, 163,  53, 283, 178, 219, 200, 113, 113,  63, 114,\n",
       "       126, 274,  88, 311,  83,  71, 134, 244,  65, 173,  57,  68, 141,\n",
       "       270, 134, 202, 148,  64, 302], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data,test_size=0.25,random_state=42)\n",
    "\n",
    "X_train = train.iloc[:, :-1]\n",
    "X_test = test.iloc[:, :-1]\n",
    "y_train = train['Y'].values\n",
    "y_test = test['Y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gaussian Process Regression initial result\n",
    "\n",
    "Train a Gaussian Process with a composite kernel formed as:\n",
    "\n",
    "$$\n",
    "\\kappa_1(\\mathbf x_i, \\mathbf x_j) = \\kappa_c(\\mathbf x_i, \\mathbf x_j)\\times\\kappa_r(\\mathbf x_i, \\mathbf x_j) + \\kappa_w(\\mathbf x_i, \\mathbf x_j)\n",
    "$$ where\n",
    "- $\\kappa_c(\\mathbf x_i, \\mathbf x_j)$ is a constant kernel\n",
    "- $\\kappa_r(\\mathbf x_i, \\mathbf x_j)$ is an isotropic RBF kernel with length_scale $l$: \n",
    "$$\n",
    "\\kappa_r(\\mathbf x_i, \\mathbf x_j) = \\exp\\left( -\\frac{\\|\\mathbf x_i - \\mathbf x_j\\|^2}{2l^2}\\right)\n",
    "$$\n",
    "- $\\kappa_w(\\mathbf x_i, \\mathbf x_j)$ is a WhiteKernel that explains the additive noise component\n",
    "$$\n",
    "\\kappa_w(\\mathbf x_i, \\mathbf x_j) = \\left \\{ \\begin{array}{ll} \\sigma_n^2 & \\mbox{if } \\mathbf x_i== \\mathbf x_j \\\\ 0 & \\mbox{otherwise} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Choose the following initial parameters for these kernels:\n",
    "  - RBF kernel:\n",
    "     - `length_scale`= 1.5\n",
    "     - `length_scale_bounds` [1e-2, 1e3]\n",
    "  - White noise kernel:\n",
    "     - `noise_level`=0.1\n",
    "     - `noise_level_bounds` [1e-10, 1e6]\n",
    "     \n",
    "**Print the performance of the model in the test set.**\n",
    "\n",
    "**Print the values of the kernel parameters after the GP optimization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alonso\\Nueva carpeta\\Lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:429: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__k2__length_scale is close to the specified upper bound 1000.0. Increasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianProcessRegressor(kernel=7.07**2 * RBF(length_scale=1.5) + WhiteKernel(noise_level=0.1),\n",
       "                         n_restarts_optimizer=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianProcessRegressor</label><div class=\"sk-toggleable__content\"><pre>GaussianProcessRegressor(kernel=7.07**2 * RBF(length_scale=1.5) + WhiteKernel(noise_level=0.1),\n",
       "                         n_restarts_optimizer=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianProcessRegressor(kernel=7.07**2 * RBF(length_scale=1.5) + WhiteKernel(noise_level=0.1),\n",
       "                         n_restarts_optimizer=10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, WhiteKernel, Sum, Product\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "#defining the kernels to use\n",
    "constant_kernel = ConstantKernel(constant_value=50.0, constant_value_bounds=\"fixed\")\n",
    "rbf_kernel = RBF(length_scale=1.5, length_scale_bounds=[1e-2, 1e3])\n",
    "white_kernel = WhiteKernel(noise_level=0.1, noise_level_bounds=[1e-10, 1e6])\n",
    "#Linearly combining\n",
    "aux_kernel = Product(constant_kernel, rbf_kernel)\n",
    "custom_kernel = Sum(aux_kernel, white_kernel)\n",
    "#Training\n",
    "my_model_GPR = GaussianProcessRegressor(kernel=custom_kernel, n_restarts_optimizer=10)\n",
    "my_model_GPR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99.22913774 99.76540806 99.55879399 99.74382225 99.78526854 99.46229161\n",
      " 99.82244937 99.46777914 99.69352509 99.76504409 99.73704141 99.50010539\n",
      " 99.54815834 99.83240138 99.75649552 99.53237577 99.81879697 99.81061938\n",
      " 99.04534983 99.83849271 99.74429506 99.71590234 99.73650881 99.84120112\n",
      " 99.78672445 99.79427191 99.80755842 99.70622858 99.62534863 99.79954403\n",
      " 99.80167195 99.76378967 99.79774948 99.82063712 99.7139137  99.63070747\n",
      " 99.81550041 99.80679103 99.81087637 99.72880532 99.78942719 99.77850026\n",
      " 99.59561009 99.78651614 99.79029049 99.72665312 99.64739145 99.47297153\n",
      " 99.68982476 99.75788547 99.47811617 99.68193793 99.76894123 99.78641145\n",
      " 99.46233249 99.75355361 99.78510344 99.71492221 99.77064901 99.43471504\n",
      " 99.57593203 99.7333818  99.6334059  99.81145391 99.79804674 99.72265368\n",
      " 99.69809763 99.81341811 99.4285622  99.7675396  99.75440184 99.78558586\n",
      " 99.75738101 99.67963249 99.71410158 99.72611943 99.58585024 99.82531674\n",
      " 99.83635561 99.82180362 99.78536852 99.60893142 99.68667922 99.70710957\n",
      " 99.7935113  99.67468738 99.2207206  99.56435943 99.72824218 99.79043861\n",
      " 99.79259422 99.79109897 99.51943643 99.59264645 99.60376413 99.81912677\n",
      " 99.81745139 99.78553748 99.71657756 99.76880262 99.83160966 99.39462786\n",
      " 99.81191294 99.48810528 99.82450818 99.39380494 99.812505   99.83137297\n",
      " 99.663596   99.63641549 99.76654674] [219  70 202 230 111  84 242 272  94  96  94 252  99 297 135  67 295 264\n",
      " 170 275 310  64 128 232 129 118 263  77  48 107 140 113  90 164 180 233\n",
      "  42  84 172  63  48 108 156 168  90  52 200  87  90 258 136 158  69  72\n",
      " 171  95  72 151 168  60 122  52 187 102 214 248 181 110 140 202 101 222\n",
      " 281  61  89  91 186 220 237 233  68 190  96  72 153  98  37  63 184 144\n",
      " 150 280 125  59  65 281 277 167  90  72 178  88 270 101 197  97  53  71\n",
      " 262  52 102]\n"
     ]
    }
   ],
   "source": [
    "y_pred, sigma = my_model_GPR.predict(X_test, return_std=True)\n",
    "print(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 65.75276768675705\n",
      "Mean Squared Error (MSE): 7627.801568220912\n",
      "R-squared (R2): -0.37942666713590567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# Calculate and print performance metrics \n",
    "#This can give us extra insight\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is not good For starters it always predict the same number, and it is extremely far from the true result.\n",
    "Also the MSE is skyrocketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Kernel Parameters:\n",
      "k1: 7.07**2 * RBF(length_scale=1e+03)\n",
      "k2: WhiteKernel(noise_level=9.05e+03)\n",
      "k1__k1: 7.07**2\n",
      "k1__k2: RBF(length_scale=1e+03)\n",
      "k1__k1__constant_value: 50.0\n",
      "k1__k1__constant_value_bounds: fixed\n",
      "k1__k2__length_scale: 999.9999999999998\n",
      "k1__k2__length_scale_bounds: [0.01, 1000.0]\n",
      "k2__noise_level: 9046.057170630907\n",
      "k2__noise_level_bounds: [1e-10, 1000000.0]\n"
     ]
    }
   ],
   "source": [
    "optimized_kernel = my_model_GPR.kernel_\n",
    "\n",
    "# Get the parameters of the optimized kernel\n",
    "kernel_params = optimized_kernel.get_params()\n",
    "\n",
    "# Print the values of the kernel parameters\n",
    "print(\"Optimized Kernel Parameters:\")\n",
    "for param, value in kernel_params.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss about the differences between the kernel parameters before and after optimizing the GP.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Things worth noticing with these parameters\n",
    "- The noise level increases from 0.1 to 9031, this is 4 orders of magnitude. We use a really noisy kernel\n",
    "- The length scale increases by 999 units, it increases so much that it almost needs to go out of bounds\n",
    "- When the optimizer goes so near the constraint we may notice that something is 'funny'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Strategies to improve the initial result\n",
    "\n",
    "In this assignment we are going to explore three strategies to improve this initial result\n",
    "\n",
    "1. Scaling the data\n",
    "2. Feature selection\n",
    "3. Kernel design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Scaling the data\n",
    "\n",
    "Repeat the experiment that produced the baseline result scaling the observations with a `MinMaxScaler` and evaluate the impact of this scaling in the performance of the GP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Did scaling improve the accuracy of the GP?**\n",
    "\n",
    "**Did scaling affect to the final value of the kernel parameters after the optimization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvtbdKOMohFY"
   },
   "source": [
    "## 2.2 Feature selection\n",
    "\n",
    "The goal in this strategy is to study\n",
    "- if any of the variables is noisy (its presence worsens the performance of the regressors)\n",
    "- if any of the variables is not relevant (its presence or absence does not affect the performance of the regressor, hence you could save resources by skipping its measure\n",
    "- if some of these variables are more critical than the others in the conformation of the score. This way you can gain insights about the main drivers of the disease.\n",
    "\n",
    "We will explore two strategies to perform the feature selection\n",
    "\n",
    "1. Random Forests property `feature_importances_`.  \n",
    "\n",
    "2. GP with an ARD kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Random Forests `feature_importances_`\n",
    "\n",
    "In Random Forest the variables are individually selected to design the stump test in each branch node of each tree in the forest. Relevant variables will be in general oftenly selected for these test, while noisy or redundant variables will be selected less oftenly.  Besides, since the growing of each tree only considers a subset of the training data, the left-out subset can be used as validation set to evaluate the quality of each stump. In this sense, the most relevant variables will lead to better quality stumps.\n",
    "\n",
    "In the sklearn implementation of Random Forest there is a property `feature_importances_` that is precisely a score in the relevance of the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell write code that\n",
    " 1. Train a Random Forest Regressor with its hyperparameters selected by cross-validation within the following  ranges\n",
    "  - number of trees: 10, 20, 50, 100, 200, 500, 1000\n",
    "  - maximum number of leaves per node: 5, 10, 20, 50\n",
    "  \n",
    " 2. Print the score in the test set  of the Random Forest fitted with the best set of hyperparameters\n",
    " \n",
    " 3. Print the value of `feature_importances_` for each feature in the data set\n",
    " \n",
    " 4. Sort the features in order of decreasing importance in an array called `random_forest_order`\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Did RF perform in the test set better than GP?**\n",
    "\n",
    "**What are the more relevant features according to RF?**\n",
    "\n",
    "**Are there significant differences in relevance among the features?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell write code that implements a `for loop` that in each iteration trains a GP with the settings of Section 1 but increasing the number of features in the ordering suggested by `random_forest_order`. \n",
    "\n",
    "Plot the GP accuracy in the test set vs. the number of features used to model the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best number of features to model the problem?**\n",
    "\n",
    "**Does removing features improve the performance of RF?**\n",
    "\n",
    "**Are there noisy features? (Features that, if present, significantly worsen the performance of the GP)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 GP with an ARD kernel\n",
    "\n",
    "The fitting of a GP endowed with an anisotropic RBF kernel obtains a different value of the `length_scale` for each variable.\n",
    "\n",
    "**Relate the length scale of each variable with its relevance in the predictive function**\n",
    "\n",
    "Hint: Consider how does the output of the predictive function changes as the value of a certain variable $x_k$ changes depending on $l_k^2$.\n",
    "\n",
    "In the next cell write code that fits a GP with an ARD kernel. \n",
    "\n",
    "**Print the lengthscale value of each feature after the kernel has been optimized** Hint, learn to use `kernel_get_params()`.** \n",
    "\n",
    "**Sort the features in order of decreasing importance in an array called `ARD_order`**\n",
    "\n",
    "**Print the score in the test set  of the GP with ARD kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell write code that implements a `for loop` that in each iteration trains a GP with an ARD kernel but increasing the number of features in the ordering suggested by `ARD_order`. \n",
    "\n",
    "Plot the GP accuracy in the test set vs. the number of features used to model the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best number of features to model the problem according to the ARD kernel?**\n",
    "\n",
    "**How stable are the optimizations of the Gaussian Processes with ARD kernels as the number of features increase?**\n",
    "\n",
    "**Does removing features improve the performance of GPs?**\n",
    "\n",
    "**Are there noisy features? (Features that, if present, significantly worsen the performance of the GP)**\n",
    "\n",
    "**How does the feature selection suggested by the ARD kernel compare with that suggested by random forest?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnqv8kpOdLzC"
   },
   "source": [
    "# 4. Exploring sophisticate kernels for the GP\n",
    "\n",
    "The greatest potentiality of GPs are the exploration of different kernels that capture the geometry of the inputs. \n",
    "\n",
    "Besides, the essential kernels can be combined into more sophisticate ones using the addition and multiplication operations.\n",
    "\n",
    "And the most interesting feature, the GP implementation is able to optimize the parameters of the kernel maximizing the likelihood of the observations, what saves the crossvalidation step for optimizing parameters.\n",
    "\n",
    "Read the [section 1.7.5 of this site](https://scikit-learn.org/stable/modules/gaussian_process.html) to learn the different kernels that are implemented in the scikit learn distribution of Gaussian Processes.\n",
    "\n",
    "In this section check at least twenty different kernel configurations and evaluate if they improve the kernel evaluated in section 1. Remember this kernel was\n",
    "\n",
    "$$\n",
    "\\kappa_1(\\mathbf x_i, \\mathbf x_j) = \\kappa_c(\\mathbf x_i, \\mathbf x_j)\\times\\kappa_r(\\mathbf x_i, \\mathbf x_j) + \\kappa_w(\\mathbf x_i, \\mathbf x_j)\n",
    "$$ where\n",
    "- $\\kappa_c(\\mathbf x_i, \\mathbf x_j)$ is a constant kernel\n",
    "- $\\kappa_r(\\mathbf x_i, \\mathbf x_j)$ is an isotropic RBF kernel with length_scale $l$: \n",
    "$$\n",
    "\\kappa_r(\\mathbf x_i, \\mathbf x_j) = \\exp\\left( -\\frac{\\|\\mathbf x_i - \\mathbf x_j\\|^2}{2l^2}\\right)\n",
    "$$\n",
    "- $\\kappa_w(\\mathbf x_i, \\mathbf x_j)$ is a WhiteKernel that explains the additive noise component\n",
    "$$\n",
    "\\kappa_w(\\mathbf x_i, \\mathbf x_j) = \\left \\{ \\begin{array}{ll} \\sigma_n^2 & \\mbox{if } \\mathbf x_i== \\mathbf x_j \\\\ 0 & \\mbox{otherwise} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Within the kernel combinations to explore you can include:\n",
    "1. Replace $\\kappa_r(\\mathbf x_i, \\mathbf x_j)$ by an anisotropic RBF in $\\kappa_1(\\mathbf x_i, \\mathbf x_j)$. \n",
    "\n",
    "3. Individual kernels presented in the lecture\n",
    "\n",
    "4. Addition of several kernels\n",
    "\n",
    "5. Multiplication of several kernels\n",
    "\n",
    "6. Use your imagination!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the different kernel combinations to characterize how difficult is the problem at hand in terms of how difficult is to find out a kernel that achieves the best possible result in the test set.\n",
    "\n",
    "For this purpose:\n",
    "1. Group in a same array all the scores in the **test set** achieved by all the kernel combinations that you explore in this section. Consider carrying out this exploration in a programatic fashion. As a suggestion, program nested loops that create composite kernels as combination of simple kernels.\n",
    "\n",
    "2. Discuss about the range of test accuracies that can be reached with GPs when the kernel is more carefully designed. Depending on the number of different kernels explored you might consider adding to your discussion\n",
    "- minimum, maximum, mean values\n",
    "- standard deviations\n",
    "- percentiles\n",
    "- histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382147,
     "status": "ok",
     "timestamp": 1604483138700,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "-w5KJnordc0Y",
    "outputId": "a7e7438b-864a-4559-d112-91af32f60bb1"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 389618,
     "status": "ok",
     "timestamp": 1604483146195,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "7xyVyI6Yy76w"
   },
   "source": [
    "# Items for discussion\n",
    "- Which strategy turned out to be the best in terms of increasing the performance of the GP? \n",
    "- Did this strategy performed significantly better than the others?\n",
    "- Kernel design pushes the GP model further into the **black box method** region, what is the price you pay for sticking to the more interpretable ARD kernel in terms of accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
